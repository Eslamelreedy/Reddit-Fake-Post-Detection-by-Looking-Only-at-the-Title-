{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3V_XshYSxVt",
        "outputId": "d376b318-db0f-472a-9c86-8704a417a671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libiraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#read train and test data\n",
        "data=pd.read_csv('/content/drive/MyDrive/comptition3/x_train.csv',index_col='id')\n",
        "test_data=pd.read_csv('/content/drive/MyDrive/comptition3/x_test.csv')"
      ],
      "metadata": {
        "id": "HWsooIeVU5A9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "zRadOopGVuzL",
        "outputId": "9b6fb933-f0ec-4217-c730-92dd082f53ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                     text  label\n",
              "id                                                              \n",
              "265723  A group of friends began to volunteer at a hom...      0\n",
              "284269  British Prime Minister @Theresa_May on Nerve A...      0\n",
              "207715  In 1961, Goodyear released a kit that allows P...      0\n",
              "551106  Happy Birthday, Bob Barker! The Price Is Right...      0\n",
              "8584    Obama to Nation: 聙\"Innocent Cops and Unarmed Y...      0\n",
              "117912  In the 1920鈥檚, Hitler was forbidden to address...      0\n",
              "213064  Nerd Wins Scrabble with word you've never hear...      0\n",
              "398923  Why 95.8% of Female Newscasters Have the Same ...      1\n",
              "314798  Donald Trump Says He'll Do This If More 'Inapp...      0\n",
              "20243   5 crazy facts about Lamborghini's outrageous e...      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3aaf46e7-89b8-4360-8b51-d5293b2f296e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>265723</th>\n",
              "      <td>A group of friends began to volunteer at a hom...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284269</th>\n",
              "      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207715</th>\n",
              "      <td>In 1961, Goodyear released a kit that allows P...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>551106</th>\n",
              "      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8584</th>\n",
              "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117912</th>\n",
              "      <td>In the 1920鈥檚, Hitler was forbidden to address...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213064</th>\n",
              "      <td>Nerd Wins Scrabble with word you've never hear...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398923</th>\n",
              "      <td>Why 95.8% of Female Newscasters Have the Same ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314798</th>\n",
              "      <td>Donald Trump Says He'll Do This If More 'Inapp...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20243</th>\n",
              "      <td>5 crazy facts about Lamborghini's outrageous e...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3aaf46e7-89b8-4360-8b51-d5293b2f296e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3aaf46e7-89b8-4360-8b51-d5293b2f296e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3aaf46e7-89b8-4360-8b51-d5293b2f296e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "sXKBP-B9WDu3",
        "outputId": "372eca06-5d6b-4efb-ee11-f950eb4db102"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                               text\n",
              "0   0                                         stargazer \n",
              "1   1                                               yeah\n",
              "2   2  PD: Phoenix car thief gets instructions from Y...\n",
              "3   3  As Trump Accuses Iran, He Has One Problem: His...\n",
              "4   4                       \"Believers\" - Hezbollah 2011\n",
              "5   5           The rise of Italian fascism (circa 1925)\n",
              "6   6  Trump鈥檚 pick to lead ICE, who touted surge in ...\n",
              "7   7                                 My friend mid-flip\n",
              "8   8  Look at this cool iPhone case I got the other day\n",
              "9   9                  My rope flying off a friend swing"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-afe327e5-7354-423d-951b-a980316b1658\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>stargazer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>yeah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>PD: Phoenix car thief gets instructions from Y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>As Trump Accuses Iran, He Has One Problem: His...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>\"Believers\" - Hezbollah 2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>The rise of Italian fascism (circa 1925)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>Trump鈥檚 pick to lead ICE, who touted surge in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>My friend mid-flip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>Look at this cool iPhone case I got the other day</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>My rope flying off a friend swing</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-afe327e5-7354-423d-951b-a980316b1658')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-afe327e5-7354-423d-951b-a980316b1658 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-afe327e5-7354-423d-951b-a980316b1658');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking of the no. of nulls in each columns\n",
        "data.isnull().sum().sort_values(ascending=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIp_iBtLWMNr",
        "outputId": "aee0947d-eba5-4aee-fc85-316c659ea874"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text     0\n",
              "label    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#display data information\n",
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nObPAYTWOsM",
        "outputId": "a920a1b5-0f85-4819-d3ab-ba15ef84a727"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 60000 entries, 265723 to 34509\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    60000 non-null  object\n",
            " 1   label   60000 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 1.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check if there is duplicate in data\n",
        "data.duplicated().sum()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CddT7UJ7WrJH",
        "outputId": "d0a2bac3-eacb-4c21-f0df-29353efc1370"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "345"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preprocessing data"
      ],
      "metadata": {
        "id": "gq3dGocuWxJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicate rows\n",
        "data.drop_duplicates(subset='text', inplace=True)\n"
      ],
      "metadata": {
        "id": "uzzmawX2Wtm9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replace label 2 with label 1\n",
        "data[\"new_label\"] = 0\n",
        "data.loc[data[\"label\"] == 0, \"new_label\"] = 0\n",
        "data.loc[data[\"label\"] > 0, \"new_label\"] = 1\n",
        "\n",
        "#checking for the distribution of grade_bad\n",
        "data['new_label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh0wyEVgW6rg",
        "outputId": "25e9871b-e761-45b7-d6c3-5dc2cfd90396"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    31949\n",
              "1    27696\n",
              "Name: new_label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#display new data shape\n",
        "data.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7464TY4RW_Ac",
        "outputId": "2f5fe9ac-b63c-48ef-fbd2-291351041a29"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(59645, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from bokeh.io import output_notebook\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import re\n",
        "import pickle\n",
        "import sklearn\n",
        "import holoviews as hv\n",
        "import nltk \n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "from pathlib import Path\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4h2jN7aXKse",
        "outputId": "a4967c6f-2921-43fd-b0f1-6a1b385fc0ca"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#use lemmatizer to convert the word to root word\n",
        "#use stop words to remove words that not add much value to the meaning of text\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "\n",
        "def clean_text(text, for_embedding=False):\n",
        "\n",
        "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
        "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
        "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
        "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
        "    if for_embedding:\n",
        "        # Keep punctuation\n",
        "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
        "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
        "\n",
        "    text = re.sub(RE_TAGS, \" \", text)\n",
        "    text = re.sub(RE_ASCII, \" \", text)\n",
        "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
        "    text = re.sub(RE_WSPACE, \" \", text)\n",
        "    #use textblob to correct spelling mistakes\n",
        "    #m = TextBlob(text)\n",
        "    #text=m.correct()\n",
        "\n",
        "    #divide rows into sentences(tokens) to be easier to understod\n",
        "    word_tokens = word_tokenize(text)\n",
        "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
        "\n",
        "    if for_embedding:\n",
        "        # no lemmatizing, lowering and punctuation / stop words removal\n",
        "        words_filtered = word_tokens\n",
        "    else:\n",
        "        words_filtered = [\n",
        "            lemmatizer.lemmatize(word) for word in words_tokens_lower if word not in stop_words\n",
        "        ]\n",
        "\n",
        "    text_clean = \" \".join(words_filtered)\n",
        "    return text_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TI_kx9WXOaJ",
        "outputId": "36488e60-e7c9-4638-db87-49dccea9630e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.text.describe()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwgalFSsXUK-",
        "outputId": "bcc13b24-0084-4086-83b7-9061edde4763"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count                                                 59645\n",
              "unique                                                59645\n",
              "top       A group of friends began to volunteer at a hom...\n",
              "freq                                                      1\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check if train data balanced or not\n",
        "data[\"new_label\"].value_counts(normalize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUrso_ibXXx1",
        "outputId": "446a3392-54b1-4a76-cc09-4e8179a6cd93"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.535653\n",
              "1    0.464347\n",
              "Name: new_label, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split columns\n",
        "test=test_data['text']\n",
        "x=data['text']\n",
        "y=data['new_label']\n",
        "x,y,test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmBBQgY_Xbzb",
        "outputId": "0cf437de-f7fb-44ca-c510-f088973510c4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(id\n",
              " 265723    A group of friends began to volunteer at a hom...\n",
              " 284269    British Prime Minister @Theresa_May on Nerve A...\n",
              " 207715    In 1961, Goodyear released a kit that allows P...\n",
              " 551106    Happy Birthday, Bob Barker! The Price Is Right...\n",
              " 8584      Obama to Nation: 聙\"Innocent Cops and Unarmed Y...\n",
              "                                 ...                        \n",
              " 70046     Finish Sniper Simo H盲yh盲 during the invasion o...\n",
              " 189377    Nigerian Prince Scam took $110K from Kansas ma...\n",
              " 93486     Is It Safe To Smoke Marijuana During Pregnancy...\n",
              " 140950    Julius Caesar upon realizing that everyone in ...\n",
              " 34509     Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New...\n",
              " Name: text, Length: 59645, dtype: object,\n",
              " id\n",
              " 265723    0\n",
              " 284269    0\n",
              " 207715    0\n",
              " 551106    0\n",
              " 8584      0\n",
              "          ..\n",
              " 70046     0\n",
              " 189377    1\n",
              " 93486     0\n",
              " 140950    0\n",
              " 34509     1\n",
              " Name: new_label, Length: 59645, dtype: int64,\n",
              " 0                                               stargazer \n",
              " 1                                                     yeah\n",
              " 2        PD: Phoenix car thief gets instructions from Y...\n",
              " 3        As Trump Accuses Iran, He Has One Problem: His...\n",
              " 4                             \"Believers\" - Hezbollah 2011\n",
              "                                ...                        \n",
              " 59146                    Bicycle taxi drivers of New Delhi\n",
              " 59147    Trump blows up GOP's formula for winning House...\n",
              " 59148    Napoleon returns from his exile on the island ...\n",
              " 59149     Deep down he always wanted to be a ballet dancer\n",
              " 59150    Toddler miraculously survives 6-story fall lan...\n",
              " Name: text, Length: 59151, dtype: object)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Models"
      ],
      "metadata": {
        "id": "bLfQIh-EsvhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "DCxcm5nTXd2r"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV\n",
        "from sklearn.model_selection import PredefinedSplit"
      ],
      "metadata": {
        "id": "Fw-sAn6GZVyb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Attempt 1\n"
      ],
      "metadata": {
        "id": "9-4ASY_WvMq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the original dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(x, y, train_size=0.8, stratify=y, random_state=2022)\n",
        "\n",
        "# Create a list of indices for each data point indicating whether it belongs to the training or validation set\n",
        "split_index = [-1 if i in X_train.index else 0 for i in x.index]\n",
        "\n",
        "# Use the list to create a PredefinedSplit object for GridSearchCV\n",
        "pds = PredefinedSplit(test_fold=split_index)\n",
        "\n",
        "# Define the pipeline with TfidfVectorizer and LogisticRegression\n",
        "pipe = Pipeline([\n",
        "    ('cvec', TfidfVectorizer(preprocessor=clean_text, analyzer=\"word\", max_df=0.3, min_df=10, norm=\"l2\")),\n",
        "    ('lr', LogisticRegression(solver='sag'))\n",
        "])\n",
        "\n",
        "# Define the hyperparameters to tune using GridSearchCV\n",
        "pipe_params = {\n",
        "    'cvec__ngram_range': [(1,1), (2,2), (1,3)],\n",
        "    'lr__C': [0.01, 0.1, 1],\n",
        "    'lr__penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object with the pipeline and hyperparameters\n",
        "gs = GridSearchCV(pipe, param_grid=pipe_params, scoring=\"roc_auc\", cv=pds)\n",
        "\n",
        "# Train the model with GridSearchCV using the entire dataset\n",
        "gs.fit(x, y)\n",
        "\n",
        "# Print the best train score and the best parameters found by GridSearchCV\n",
        "print(\"Train score:\", gs.score(x, y))\n",
        "print(\"Best parameters:\", gs.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SY92EfSeZesn",
        "outputId": "0729c041-7dc8-4c15-86e6-3e10d4530e89"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "9 fits failed out of a total of 18.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "9 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.82967659        nan 0.85444985        nan 0.86925363\n",
            "        nan 0.69056803        nan 0.69705763        nan 0.70115296\n",
            "        nan 0.83105877        nan 0.85672237        nan 0.87357914]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train score 0.920156544987508\n",
            "Best params: {'cvec__ngram_range': (1, 3), 'lr__C': 1, 'lr__penalty': 'l2'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict output and save submission\n",
        "submission = pd.DataFrame()\n",
        "submission['id'] = test_data['id']\n",
        "submission['label']=gs.predict_proba(test)[:,1]\n",
        "submission.to_csv('Logistic_Reg.csv', index=False)"
      ],
      "metadata": {
        "id": "z3wyJVoma9qu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 2"
      ],
      "metadata": {
        "id": "GdZPXfb6vSTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest pipeline setup\n",
        "rf_pipe = Pipeline([\n",
        "    ('tvec', TfidfVectorizer(preprocessor=clean_text, analyzer=\"char\", max_df=0.3, min_df=10, norm=\"l2\")),\n",
        "    ('rf', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Setting up Random Forest hyperparameters\n",
        "rf_params = {\n",
        "    'tvec__max_features': [2000],\n",
        "    'tvec__ngram_range': [(1, 2)],\n",
        "    'tvec__stop_words': ['english'],\n",
        "    'rf__max_depth': [1000],\n",
        "    'rf__min_samples_split': [100],\n",
        "    'rf__max_leaf_nodes': [None]\n",
        "}\n",
        "\n",
        "# Fit Random Forest pipeline to the data\n",
        "rf_pipe.fit(x, y)\n",
        "\n",
        "# Setting up GridSearch for TFIDFVectorizer\n",
        "rf_gs = GridSearchCV(rf_pipe, param_grid=rf_params, scoring=\"roc_auc\", cv=pds, verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fitting Random Forest with GridSearchCV\n",
        "rf_gs.fit(x, y)\n",
        "\n",
        "# Print train score and best params\n",
        "print(\"Train score:\", rf_gs.score(x, y))\n",
        "print(\"Best params:\", rf_gs.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCOStrdabK_e",
        "outputId": "465c1570-d254-43b0-e4e6-9802598d501c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train score 0.9803000126899243\n",
            "Best params: {'rf__max_depth': 1000, 'rf__max_leaf_nodes': None, 'rf__min_samples_split': 100, 'tvec__max_features': 2000, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict output and save submission\n",
        "submission = pd.DataFrame()\n",
        "submission['id'] = test_data['id']\n",
        "submission['label']=rf_gs.predict_proba(test)[:,1]\n",
        "submission.to_csv('Rand_Forest.csv', index=False)\n",
        "     "
      ],
      "metadata": {
        "id": "AxlrVNKadj8r"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Attempt 3"
      ],
      "metadata": {
        "id": "m_OJXhwrvXn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# combine the preprocessor with the model as a full tunable pipeline\n",
        "full_pipeline = Pipeline(steps=[\n",
        "    ('cvec', TfidfVectorizer(\n",
        "        preprocessor=clean_text, # text cleaning function\n",
        "        analyzer=\"word\", # analyze at the word level\n",
        "        max_df=0.3, # ignore words that occur in more than 30% of the documents\n",
        "        min_df=10, # ignore words that occur in less than 10 documents\n",
        "        norm=\"l2\" # normalize the vectors using the l2 norm\n",
        "    )),\n",
        "    ('my_classifier', XGBClassifier(\n",
        "        n_estimators=200, # number of trees in the forest\n",
        "        colsample_bytree=0.8, # subsampling ratio of columns when constructing each tree\n",
        "        subsample=0.8, # subsampling ratio of the training instances\n",
        "        nthread=10, # number of parallel threads used to run XGBoost\n",
        "        learning_rate=0.1 # shrinkage factor to prevent overfitting\n",
        "    ))\n",
        "])\n",
        "\n",
        "# fit the pipeline on the training data\n",
        "full_pipeline.fit(x, y)\n",
        "\n",
        "# define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'cvec__ngram_range': [(1,1), (2,2), (1,3)], # range of n-gram sizes\n",
        "    'my_classifier__max_depth': [5, 10, 20, 30, 40, 50] # maximum depth of each tree\n",
        "}\n",
        "\n",
        "# perform grid search cross-validation to find the best hyperparameters\n",
        "grid_search = GridSearchCV(\n",
        "    full_pipeline, # pipeline object\n",
        "    param_grid=param_grid, # hyperparameter grid\n",
        "    scoring=\"roc_auc\", # use ROC AUC as the performance metric\n",
        "    cv=pds, # use the PredefinedSplit object to split the data\n",
        "    verbose=1, # print progress messages\n",
        "    n_jobs=-1 # use all available CPUs for parallel processing\n",
        ")\n",
        "grid_search.fit(x, y)\n",
        "\n",
        "# print the best cross-validation score and hyperparameters\n",
        "print('best score {}'.format(grid_search.best_score_))\n",
        "print('best params {}'.format(grid_search.best_params_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obdwlKNXdmip",
        "outputId": "ed11e2c3-1085-4f94-8cbb-792f31a11135"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 1 folds for each of 18 candidates, totalling 18 fits\n",
            "best score 0.8551031369255028\n",
            "best score {'cvec__ngram_range': (1, 3), 'my_classifier__max_depth': 50}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict output and save submission\n",
        "submission = pd.DataFrame()\n",
        "submission['id'] = test_data['id']\n",
        "submission['label']=grid_search.predict_proba(test)[:,1]\n",
        "submission.to_csv('XGB_grid.csv', index=False)"
      ],
      "metadata": {
        "id": "zDNp-5Y_dtYe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 4 \n"
      ],
      "metadata": {
        "id": "qchzLSwBvaYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a pipeline that consists of a TfidfVectorizer and an MLPClassifier\n",
        "full_pipline_5 = Pipeline(steps=[('cvec', TfidfVectorizer(preprocessor=clean_text, analyzer=\"word\", max_df=0.3, min_df=10, norm=\"l2\")), \n",
        "                                 ('my_classifier', MLPClassifier(random_state=1, solver=\"adam\", hidden_layer_sizes=(224, 120, 12,), activation=\"relu\", n_iter_no_change=10))])\n",
        "\n",
        "# Set the hyperparameters for the TfidfVectorizer\n",
        "param_grid_5 = {'cvec__ngram_range': [(1,1), (2,2), (1,3)]}\n",
        "\n",
        "# Perform a grid search using the pipeline and hyperparameters defined above\n",
        "grid_mlp = GridSearchCV(full_pipline_5, param_grid_5, scoring=\"roc_auc\", cv=pds, verbose=1, n_jobs=-1)\n",
        "grid_mlp.fit(x, y)\n",
        "\n",
        "# Print the best score and hyperparameters found by the grid search\n",
        "print('best score {}'.format(grid_mlp.best_score_))\n",
        "print('best params {}'.format(grid_mlp.best_params_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9k4bGt-dw4J",
        "outputId": "fffe59b3-e25b-4322-e0b6-de4a95224da6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 1 folds for each of 3 candidates, totalling 3 fits\n",
            "best score 0.8509244449869061\n",
            "best score {'cvec__ngram_range': (1, 1)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#predict output and save submission\n",
        "submission = pd.DataFrame()\n",
        "\n",
        "submission['id'] = test_data['id']\n",
        "submission['label']=grid_mlp.predict_proba(test)[:,1]\n",
        "submission.to_csv('MLP_Grid.csv', index=False)"
      ],
      "metadata": {
        "id": "Z8GR7wcod3Uy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 5"
      ],
      "metadata": {
        "id": "Esn5FTGnvlk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# set vectorizer and classifier hyperparameters as a pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('cvec', TfidfVectorizer(preprocessor=clean_text,analyzer=\"word\", max_df=0.3, min_df=10, norm=\"l2\")),    \n",
        "    ('nb', MultinomialNB())\n",
        "])\n",
        "\n",
        "# define hyperparameters grid to search over\n",
        "param_grid = {\n",
        "    'cvec__ngram_range': [(1,1), (2,2), (1,3)],\n",
        "    'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "# perform grid search with cross validation\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=pipeline, \n",
        "    param_grid=param_grid, \n",
        "    scoring=\"roc_auc\", \n",
        "    cv=pds\n",
        ")\n",
        "\n",
        "# train model with grid search\n",
        "grid_search.fit(x, y)\n",
        "\n",
        "# print best score and hyperparameters\n",
        "print(\"Train score:\", grid_search.score(x, y))\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pRrTQE4kQHx",
        "outputId": "e3a0054e-3200-4d96-d00c-d62cea128562"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train score 0.8961851993624514\n",
            "Best params: {'cvec__ngram_range': (1, 3), 'nb__alpha': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict output and save submission\n",
        "submission = pd.DataFrame()\n",
        "submission['id'] = test_data['id']\n",
        "submission['label']=grid_search.predict_proba(test)[:,1]\n",
        "submission.to_csv('MultinomialNB_grid.csv', index=False)"
      ],
      "metadata": {
        "id": "a7WitaiNv7v7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5xeUplOXqNRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Questions**\n"
      ],
      "metadata": {
        "id": "NauLIZGnny4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?\n"
      ],
      "metadata": {
        "id": "RQqTwz2bn3AP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Character n-grams break down text into sequences of n consecutive characters while word n-grams break down text into sequences of n consecutive words. Character n-grams tend to suffer more from the OOV issue due to generating a larger number of features, especially when using higher values of n. Word n-grams benefit from the fact that words are more semantically meaningful than characters and can capture the overall meaning of the text."
      ],
      "metadata": {
        "id": "tpM4_JLWn03K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###What is the difference between stop word removal and stemming? Are these techniques language-dependent?\n"
      ],
      "metadata": {
        "id": "XQGpjiq-oLZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop word removal and stemming are both techniques used in natural language processing, but they serve different purposes.\n",
        "\n",
        "Stop word removal involves removing common words that do not carry much meaning in a sentence, such as \"the\", \"a\", \"an\", etc. These words are often filtered out before processing the text to reduce the computational cost and focus on the more meaningful words in the text.\n",
        "\n",
        "Stemming involves reducing words to their base or root form, by removing suffixes or prefixes, so that similar words can be treated as the same word. For example, the words \"jump\", \"jumping\", and \"jumped\" would be stemmed to \"jump\". Stemming is often used to reduce the dimensionality of the text data and improve the efficiency and effectiveness of downstream processing tasks such as information retrieval or text classification.\n",
        "\n",
        "Both stop word removal and stemming are language-dependent, as the stop words and rules for stemming may differ between languages. For example, the stop words in English may be different from the stop words in French or Arabic, and the rules for stemming English words may be different from those for stemming Arabic words."
      ],
      "metadata": {
        "id": "mHPa8NV-n-KF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Is tokenization techniques language dependent? Why?"
      ],
      "metadata": {
        "id": "Lzs6BDhUohck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization techniques are language-dependent because different languages have different rules for dividing text into tokens. For example, in English, words are typically separated by spaces, so tokenization can be done by splitting the text on spaces. However, in languages like Chinese or Japanese, there are no spaces between words, so tokenization requires more advanced techniques such as using language-specific dictionaries or statistical models to identify word boundaries. Additionally, some languages have complex writing systems, such as Arabic or Hebrew, which require special techniques to identify individual words or tokens. Therefore, the choice of tokenization technique depends on the specific language and the characteristics of its writing system, and different techniques may need to be applied to different languages to achieve optimal results.\n"
      ],
      "metadata": {
        "id": "FObDS320oen2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is the difference between count vectorizer and TF-IDF vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?\n"
      ],
      "metadata": {
        "id": "VK-7UlBVokIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count vectorizer represents each document as a vector of word frequencies, while TF-IDF vectorizer assigns a weight to each feature based on its importance in the corpus. It would not be feasible to use all possible n-grams due to the exponential growth of dimensions. To select the optimal n-gram length, one can experiment with different values and evaluate their performance on a validation set or through cross-validation, considering the specific task and text data characteristics."
      ],
      "metadata": {
        "id": "YN6TcTlxooNI"
      }
    }
  ]
}